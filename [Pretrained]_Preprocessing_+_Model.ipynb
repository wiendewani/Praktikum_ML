{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Pretrained] Preprocessing + Model.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "AP6ilRMYSXZz",
        "0AZcYX3wza3c",
        "79HvCkaDScEu"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wiendewani/Praktikum_ML/blob/putri/%5BPretrained%5D_Preprocessing_%2B_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP6ilRMYSXZz"
      },
      "source": [
        "### Download Data\n",
        "\n",
        "gak perlu di run lagi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdC49XaUe_eZ",
        "outputId": "a7a0ee74-cfee-425d-9bb8-d79dc45ec849"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYIWLOZpsunD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c81d6747-852e-4b90-ec77-0a13c266914b"
      },
      "source": [
        "! pip install kaggle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKSalaxOuWKv"
      },
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viuDuhrqufW_"
      },
      "source": [
        "! cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao9UY39MuyPZ"
      },
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f73LVxisu0gc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d22c1f81-e78e-47cc-ed8d-0a1539684a65"
      },
      "source": [
        "!kaggle datasets download -d vitaminc/cigarette-smoker-detection"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading cigarette-smoker-detection.zip to /content\n",
            "100% 2.50G/2.50G [00:34<00:00, 50.7MB/s]\n",
            "100% 2.50G/2.50G [00:34<00:00, 78.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TktupYWygXvR"
      },
      "source": [
        "import os\n",
        "import zipfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6gQbahhf5aF"
      },
      "source": [
        "local_zip = '/content/cigarette-smoker-detection.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/content/drive/MyDrive/Pembelajaran Mesin/Project ML')\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-sywx82nzkd",
        "outputId": "cc3b7a48-e895-4cb1-b4e8-c69e5eb43dac"
      },
      "source": [
        "!pip install split-folders"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: split-folders in /usr/local/lib/python3.7/dist-packages (0.4.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2jrb6kcnpQ3"
      },
      "source": [
        "!mkdir \"/content/drive/MyDrive/Pembelajaran Mesin/Project ML/dataset\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AE59GQkpnxhR",
        "outputId": "e60eaf19-e5f4-460d-efb2-b53a7b7a5432"
      },
      "source": [
        "import splitfolders\n",
        "\n",
        "splitfolders.ratio(\"/content/drive/MyDrive/Pembelajaran Mesin/Project ML/data/data\", output=\"/content/drive/MyDrive/Pembelajaran Mesin/Project ML/dataset\", seed= 1337, ratio =(.8,.2), group_prefix=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying files: 3275 files [01:39, 32.80 files/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AZcYX3wza3c"
      },
      "source": [
        "### Import Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkmTB4lNzek9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1edaaf8f-7665-4f96-b71a-51f1e1eecbee"
      },
      "source": [
        "!pip install split-folders\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from imutils import paths\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import splitfolders"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting split-folders\n",
            "  Downloading split_folders-0.4.3-py3-none-any.whl (7.4 kB)\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79HvCkaDScEu"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFmxojCGpWoL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6cac19b-7f22-453f-fead-426cb688aef7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CaIRaXHrrQp"
      },
      "source": [
        "splitfolders.ratio(\"/content/drive/MyDrive/Pembelajaran Mesin/Project ML/dataset\", \n",
        "                   output=\"/content/drive/MyDrive/Praktikum Semester 7/Modul 4/splitting\", \n",
        "                   seed= 1337, \n",
        "                   ratio =(.8,.19,.01), \n",
        "                   group_prefix=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaubfwzlHqVY"
      },
      "source": [
        "## Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### pretrained plus cnn"
      ],
      "metadata": {
        "id": "3fEgFZ449jkG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FwllnZGHv8e"
      },
      "source": [
        "base_dir = 'D:\\Wien\\Project_ML\\Dataset'\n",
        "\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'val')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "train_Not_Smoking_dir = os.path.join(train_dir, 'not_smoking')\n",
        "train_Smoking_dir = os.path.join(train_dir, 'smoking')\n",
        "\n",
        "validation_Not_Smoking_dir = os.path.join(validation_dir, 'not_smoking')\n",
        "validation_Smoking_dir = os.path.join(validation_dir, 'smoking')\n",
        "\n",
        "test_Not_Smoking_dir = os.path.join(test_dir, 'not_smoking')\n",
        "test_Smoking_dir = os.path.join(test_dir, 'smoking')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd8knbOuIa6r"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# 2.1 Gather data train & test\n",
        "train_data = []\n",
        "train_label = []\n",
        "for r, d, f in os.walk(train_dir):\n",
        "    for file in f:\n",
        "        if \".png\" in file:\n",
        "            imagePath = os.path.join(r, file)\n",
        "            image = cv2.imread(imagePath)\n",
        "            image = cv2.resize(image, (150,150))\n",
        "            train_data.append(image)\n",
        "            label = imagePath.split(os.path.sep)[-2]\n",
        "            train_label.append(label)\n",
        "\n",
        "train_data = np.array(train_data)\n",
        "train_label = np.array(train_label)\n",
        "\n",
        "val_data = []\n",
        "val_label = []\n",
        "for r, d, f in os.walk(validation_dir):\n",
        "    for file in f:\n",
        "        if \".png\" in file:\n",
        "            imagePath = os.path.join(r, file)\n",
        "            image = cv2.imread(imagePath)\n",
        "            image = cv2.resize(image, (150,150))\n",
        "            val_data.append(image)\n",
        "            label = imagePath.split(os.path.sep)[-2]\n",
        "            val_label.append(label)\n",
        "\n",
        "val_data = np.array(val_data)\n",
        "val_label = np.array(val_label)\n",
        "\n",
        "test_data = []\n",
        "test_label = []\n",
        "for r, d, f in os.walk(test_dir):\n",
        "    for file in f:\n",
        "        if \".png\" in file:\n",
        "            imagePath = os.path.join(r, file)\n",
        "            image = cv2.imread(imagePath)\n",
        "            image = cv2.resize(image, (150,150))\n",
        "            test_data.append(image)\n",
        "            label = imagePath.split(os.path.sep)[-2]\n",
        "            test_label.append(label)\n",
        "\n",
        "test_data = np.array(test_data)\n",
        "test_label = np.array(test_label)\n",
        "\n",
        "\n",
        "# Data shape\n",
        "print(\"Train Data = \", train_data.shape)\n",
        "print(\"Train Label = \", train_label.shape)\n",
        "print(\"Val Data = \", val_data.shape)\n",
        "print(\"Val Label = \", val_label.shape)\n",
        "print(\"Test Data = \", test_data.shape)\n",
        "print(\"Test Label = \", test_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.2 Normalization\n",
        "\n",
        "print(\"Before \", train_data[0][0][0])\n",
        "\n",
        "x_train = train_data.astype('float32') / 255.0\n",
        "x_test = test_data.astype('float32') / 255.0\n",
        "x_val = val_data.astype('float32') / 255.0\n",
        "\n",
        "print(\"After \", x_train[0][0][0])"
      ],
      "metadata": {
        "id": "G8xXDR_I63sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.3 Label encoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "print(\"Before \", train_label[5:])\n",
        "\n",
        "lb = LabelEncoder()\n",
        "y_train = lb.fit_transform(train_label)\n",
        "y_test = lb.fit_transform(test_label)\n",
        "y_val = lb.fit_transform(val_label)\n",
        "\n",
        "print(\"After \", y_train[5:])"
      ],
      "metadata": {
        "id": "maO0Dd_B7A4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIAQaQN6C6Uc"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class myCallbacks(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self,epoch,logs={}):\n",
        "        if (logs.get('val_acc') >= 0.85):\n",
        "            print(\"\\nReached 85% accuracy so cancelling training!\")\n",
        "            self.model.stop_training=True\n",
        "\n",
        "callback = myCallbacks()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1,\n",
        "                        height_shift_range=0.1, shear_range=0.2, \n",
        "                        zoom_range=0.8, horizontal_flip=True,\n",
        "                        fill_mode=\"nearest\")"
      ],
      "metadata": {
        "id": "ot8ty5fN8J4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pertama, kita import dulu library yang dibutuhkan\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, Input\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "\n",
        "import ssl\n",
        "\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "# Kita load model VGG16, kemudian kita potong bagian Top atau Fully Connected Layernya\n",
        "baseModel = VGG16(include_top=False, input_tensor=Input(shape=(150, 150, 3)))"
      ],
      "metadata": {
        "id": "uZqPK4QI8a-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import InputLayer, Dense, Conv2D, MaxPool2D, Flatten, Dropout\n",
        "\n",
        "class FCHeadNet:\n",
        "  def build(baseModel, classes, D):\n",
        "    headModel = baseModel.output\n",
        "#     headModel = Flatten(name=\"flatten\")(headModel)\n",
        "    headModel = Conv2D(filters=16, kernel_size=3, strides=1, padding='same', activation='elu')(headModel)\n",
        "    headModel = BatchNormalization()(headModel)\n",
        "    headModel = MaxPool2D(pool_size=2, padding='same')(headModel)\n",
        "    headModel = Conv2D(filters=32, kernel_size=3, strides=1, padding='same', activation='elu')(headModel)\n",
        "    headModel = BatchNormalization()(headModel)\n",
        "    headModel = MaxPool2D(pool_size=2, padding='same')(headModel)\n",
        "    headModel = Conv2D(filters=64, kernel_size=3, strides=1, padding='same', activation='elu')(headModel)\n",
        "    headModel = BatchNormalization()(headModel)\n",
        "    headModel = MaxPool2D(pool_size=2, padding='same')(headModel)\n",
        "    headModel = Dropout(0.25)(headModel)\n",
        "    headModel = Flatten()(headModel)\n",
        "    headModel = Dense(128, activation='elu')(headModel)\n",
        "    headModel = Dropout(0.5)(headModel)\n",
        "    headModel = Dense(1, activation='sigmoid')(headModel)\n",
        "    return headModel\n",
        "\n",
        "headModel = FCHeadNet.build(baseModel, 1, 256)\n",
        "model = Model(inputs=baseModel.input, outputs=headModel)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "YIcyF8YL8erK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_JRteUgC2x5"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model.compile(optimizer=Adam(lr=1e-3), \n",
        "              loss='binary_crossentropy', \n",
        "              metrics=['acc'])\n",
        "\n",
        "H = model.fit(aug.flow(x_train, y_train, batch_size=20), validation_data=(x_val, y_val), epochs=100, callbacks=callback)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTRKlqoSmf8V"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams['axes.spines.right'] = False\n",
        "plt.rcParams['axes.spines.top'] = False\n",
        "\n",
        "def plot_graphs_acc(history, title='Accuracy'):\n",
        "  plt.figure(figsize=(12, 7))\n",
        "  plt.title(title, fontsize=16)\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  plt.yticks(np.arange(0, 1, step=0.05))\n",
        "  plt.legend(['Training data', 'Test data'])\n",
        "  plt.show()\n",
        "\n",
        "def plot_graphs_loss(history, title='Loss'):\n",
        "  plt.figure(figsize=(12,7))\n",
        "  plt.title(title, fontsize=16)\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.yticks(np.arange(0, 4, step=0.5))\n",
        "  plt.legend(['Training data', 'Test data'])\n",
        "  plt.show()\n",
        "\n",
        "# Model\n",
        "plot_graphs_acc(H)\n",
        "print(\"\\n\")\n",
        "plot_graphs_loss(H)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBQ6JP-ZJiBY"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "pred = model.predict(x_test)\n",
        "labels = (pred > 0.5).astype(np.int)\n",
        "\n",
        "print(classification_report(y_test, labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPiyXNNPJpNx"
      },
      "source": [
        "results = confusion_matrix(np.array(y_test.argmax(axis=1)), labels.argmax(axis=1))\n",
        "fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
        "ax.matshow(results, cmap=plt.cm.Blues, alpha=0.9)\n",
        "for i in range(results.shape[0]):\n",
        "    for j in range(results.shape[1]):\n",
        "        ax.text(x=j, y=i,s=results[i, j], va='center', ha='center', size='xx-large')\n",
        " \n",
        "plt.xlabel('Prediction', fontsize=14)\n",
        "plt.ylabel('Actual Label', fontsize=14)\n",
        "plt.title('Confusion Matrix', fontsize=18)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### pretrained tapi dua"
      ],
      "metadata": {
        "id": "vzLbf3Jy9S1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pertama, kita import dulu library yang dibutuhkan\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, Input\n",
        "from tensorflow.keras.applications.xception import Xception\n",
        "\n",
        "import ssl\n",
        "\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "baseModel = VGG16(include_top=False, input_tensor=Input(shape=(150, 150, 3)))\n",
        "baseModel2 = Xception(include_top=False, input_tensor=Input(shape=(150, 150, 3)))"
      ],
      "metadata": {
        "id": "M8fia9VA9p5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = (150,150,3)\n",
        "\n",
        "vgg19 = tf.keras.applications.vgg19.VGG19(\n",
        "    input_shape=IMAGE_SIZE, weights='imagenet', include_top=False)\n",
        "for layer in vgg19.layers:\n",
        "    layer._name = layer._name + str('_19')\n",
        "    layer.trainable = False\n",
        "\n",
        "vgg16 = tf.keras.applications.vgg16.VGG16(\n",
        "    input_shape=IMAGE_SIZE, weights='imagenet', include_top=False)\n",
        "for layer in vgg16.layers:\n",
        "    layer._name = layer._name + str('_16')\n",
        "    layer.trainable = False\n",
        "\n",
        "inp = Input(IMAGE_SIZE)\n",
        "    \n",
        "vgg16_x = Flatten()(vgg16(inp))\n",
        "vgg19_x = Flatten()(vgg19(inp))\n",
        "\n",
        "x = Concatenate()([vgg16_x, vgg19_x])\n",
        "out = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(inputs = inp, outputs = out)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "qGPJ18GQ9vOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=Adam(lr=1e-3), \n",
        "              loss='binary_crossentropy', \n",
        "              metrics=['acc'])\n",
        "\n",
        "H2 = model.fit(aug.flow(X_train, y_train, batch_size=20), validation_data=(X_val, y_val), epochs=100, callbacks=callback)"
      ],
      "metadata": {
        "id": "9aOzEFoh9yeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graphs_acc(H2)\n",
        "print(\"\\n\")\n",
        "plot_graphs_loss(H2)"
      ],
      "metadata": {
        "id": "TOzaixVI-mfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "pred = model.predict(x_test)\n",
        "labels = (pred > 0.5).astype(np.int)\n",
        "\n",
        "print(classification_report(y_test, labels))"
      ],
      "metadata": {
        "id": "OVX6GaIt-oO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = confusion_matrix(np.array(y_test.argmax(axis=1)), labels.argmax(axis=1))\n",
        "fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
        "ax.matshow(results, cmap=plt.cm.Blues, alpha=0.9)\n",
        "for i in range(results.shape[0]):\n",
        "    for j in range(results.shape[1]):\n",
        "        ax.text(x=j, y=i,s=results[i, j], va='center', ha='center', size='xx-large')\n",
        " \n",
        "plt.xlabel('Prediction', fontsize=14)\n",
        "plt.ylabel('Actual Label', fontsize=14)\n",
        "plt.title('Confusion Matrix', fontsize=18)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ypGRDAgO-uG4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}